{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа 9. Работа с текстами.\n",
    "\n",
    "1. Скачайте этот ноутбук к себе.\n",
    "2. Заполните пропущенные ячейки, отвечая на заданные вопросы. Там должен быть код! (если не сказано обратное)\n",
    "3. Сохраните результат в своём гитхаб репозитории.\n",
    "4. Пришлите на почту ml-teachers ссылку на этот файл в вашем репозитории.\n",
    "5. В теме письма указать `ML Homework #12: Name Surname`. Если тема будет неправильная, то задание не будет проверено.\n",
    "6. Дедлайн: (14.03.2018 23:59)\n",
    "7. После дедлайна можно сдать, но вы получите штраф по баллам в рейтинге.\n",
    "\n",
    "Рассылка решений: 19.03.2018 17:00  \n",
    "После рассылки задания почти совсем не имеют веса.\n",
    "\n",
    "## Полезная литература\n",
    "\n",
    "- [Word2vec notebook example](https://github.com/RaRe-Technologies/movie-plots-by-genre)\n",
    "- [Sklearn text classification example](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этот раз мы будем работать с датасетом, содержащим обзоры на вина с сайта [winemag.com](https://www.winemag.com/). Сам датасет лежит на [kaggle/wine-reviews](https://www.kaggle.com/zynicide/wine-reviews/data). Скачайте файл `winemag-data-130k-v2.csv.zip`.\n",
    "\n",
    "Будем предсказывать `points` для вина по описанию. К сожалению у нас нет возможности предсказывать цену и качество конткретного вина, так как все отзывы про разные вина. Но можно предсказывать вид: `variety` (708 категорий)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk import download\n",
    "download('wordnet')\n",
    "download('averaged_perceptron_tagger')\n",
    "download('tagsets')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Подготовка данных\n",
    "\n",
    "Данных очень много и в них есть пропуски. Поэтому мы просто удаляем все объекты с пропущенными значениями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение данных\n",
    "\n",
    "- Скачайте датасет `winemag-data-130k-v2.csv.zip` с [kaggle/wine-reviews](https://www.kaggle.com/zynicide/wine-reviews/data). \n",
    "- выполните все следующие ячейки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('data/winemag-data-130k-v2.csv.zip', index_col=0)\n",
    "print(df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.dropna(subset=['points', 'price'], how='any').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y='points', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['points'] < 86), 'score'] = 'bad'\n",
    "df.loc[(df['points'] >= 86) & (df['points'] < 90), 'score'] = 'normal'\n",
    "df.loc[(df['points'] >= 90) & (df['points'] < 94), 'score'] = 'good'\n",
    "df.loc[df['points'] >= 94, 'score'] = 'excellent'\n",
    "\n",
    "sns.countplot(y='score', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000  # уменьшаем датасет\n",
    "\n",
    "df_small = pd.concat((df[df['score'] == 'bad'].head(N),\n",
    "df[df['score'] == 'normal'].head(N),\n",
    "df[df['score'] == 'good'].head(N),\n",
    "df[df['score'] == 'excellent'].head(N)))\n",
    "\n",
    "sns.countplot(y='score', data=df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим пример\n",
    "df_small[df_small['score'] == 'excellent'].sample()['description'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_small['description']\n",
    "y = df_small['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Препроцессинг текстов\n",
    "\n",
    "В этом блоке заданий вам предстоит зяняться препроцессиногом текстов.\n",
    "Самое главное в работе с текстами - их препроцессинг. При этом важно понимать, для какой модели мы \"готовим\" наш текст. Например, если мы используем просто [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), то нам не важно расположение слова в тексте и мы **должны** привести все слова к нижнему (или верхнему) регистру. Но при этом, если мы используем какую-нибудь модель, которая первое слово в тексте (или имена собственные) оценивает как-нибудь по другому, то стоит оставить такие заглавные буквы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Простая обработка данных\n",
    "В данном задании вам нужно написать функцию, разбивающую текст на слова, при этом удаляя слова короче 2 символов.\n",
    "Приведение к нижнему регистру и удаление стоп слов будет сделано с помощью аргументов конструктора конкретного векторайзера. Так работает намного быстрее.\n",
    "\n",
    "**Hint:** используйте функцию `word_tokenize` и список стоп-слов из библиотеки nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize_text(text: str) -> list:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите, что станет с этим текстом, пропустив его через токенайзер: `\"The quick brown fox jumps over the lazy dogs. Have you seen it?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tokenize_text(\"The quick brown fox jumps over the lazy dogs. Have you seen it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Стемминг текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг - процесс отбрасывания от слов \"ненужных\" частей, то есть по-сути выделение основы из слова. При этом результат стемминга может не совпадать с морфологической основой или корнем. Существует достаточно много алгоритмов стемминга. Мы рассмотрим 3 из них: (добавить Snowball Stemmer Lancaster Stemmer)\n",
    "\n",
    "Ниже предствлены примеры работы `PorterStemmer` стеммера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('hello'))\n",
    "print(stemmer.stem('cook'))\n",
    "print(stemmer.stem('cooker'))\n",
    "print(stemmer.stem('cooked'))\n",
    "print(stemmer.stem('multiply'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0\n",
    "\n",
    "Дополните функцию `simple_tokenize_text` стеммингом слов с помощью стеммера `PorterStemmer` из библиотеки nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokenize_text(text: str) -> list:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите, что станет с этим текстом, пропустив его через токенайзер: `\"The quick brown fox jumps over the lazy dogs. Have you seen it?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_tokenize_text(\"The quick brown fox jumps over the lazy dogs. Have you seen it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1\n",
    "Подберите 3 слова на английском языке, разные времена которых будут иметь разные варианты после `PorterStemmer` стемминга. То есть стемминг не будет иметь на них эффекта. Выведите результат стемминга данных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemmer.stem(??))\n",
    "print(stemmer.stem(???))\n",
    "print(stemmer.stem(???))\n",
    "print('-=-=-=-=-=-=-=-=-=-=-=')\n",
    "print(stemmer.stem(???))\n",
    "print(stemmer.stem(???))\n",
    "print(stemmer.stem(???))\n",
    "print('-=-=-=-=-=-=-=-=-=-=-=')\n",
    "print(stemmer.stem(???))\n",
    "print(stemmer.stem(???))\n",
    "print(stemmer.stem(???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобные \"проблеммы\" есть почти у всех видов стеммеров. Лемматизаторы позволяют избежать данных проблем. Лемматизация сильно похожа на стемминг, но отличаются они лишь тем, что стемминг смотрит только на слово, не учитывая контекст.\n",
    "\n",
    "Рассмотрим примеры работы лемматизатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "print(wordnet_lemmatizer.lemmatize('are'))\n",
    "print(wordnet_lemmatizer.lemmatize('is'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, `are` и `is` не превратились в начальную форму. Все из-за того, что лемматизатор не знал, к какой части речи отнести данные слова. но если мы скажем, тогда:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordnet_lemmatizer.lemmatize('are', pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize('is', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как для работы лемматизатора нам необходимы части речи, нужно научиться эти самые части речи определять. Это задача POS-tagging (POST). Рассмотрим работу POS-тэггера из библиотеки nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "posed_words = pos_tag(word_tokenize('We love our students'))\n",
    "print(posed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что же значат эти теги? Давайте разберемся!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.help import upenn_tagset\n",
    "\n",
    "print(upenn_tagset('PRP'))\n",
    "print(upenn_tagset('VBP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы объеденить лемматизатор и POS тэггер, нам потребуется промежуточная функция:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer.lemmatize(posed_words[3][0], pos=get_wordnet_pos(posed_words[3][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2\n",
    "\n",
    "Улучшим функцию `simple_tokenize_text`: добавим лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_tokenize_text(text: str) -> list:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите, что станет с этим текстом, пропустив его через токенайзер: `\"The quick brown fox jumps over the lazy dogs. Have you seen it?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_tokenize_text(\"The quick brown fox jumps over the lazy dogs. Have you seen it?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас получилось 3 способа токенизации. Будем использовать их в будующих заданиях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = [\n",
    "    simple_tokenize_text,\n",
    "    stem_tokenize_text,\n",
    "    lemm_tokenize_text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это код из практики для красивого вывода\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def examine(clf, X_train, X_test, y_train, y_test, scoring='accuracy', my_tags=['bad', 'normal', 'good', 'excellent']):\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores = clf.score(X_test, y_test)\n",
    "    sns.heatmap(confusion_matrix(y_test, clf.predict(X_test)), xticklabels=my_tags, yticklabels=my_tags, annot=True, fmt=\"d\",cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "    print('%s1 : %s2' % (clf.__class__.__name__, scores))\n",
    "    return {'score': scores, 'clf': clf.__class__.__name__}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Эксперименты с векторайзерами\n",
    "\n",
    "У нас есть несколько моделей токенайзеров, теперь можно посмотреть как каждый из них работает с разными векторайзерами: `CountVectorizer`, `TfidfVectorizer`. Вы увидите, что даже от предобработки признаков зависит много, не меньше чем от дальнейшей векторизайии. \n",
    "\n",
    "Будем сравнивать качество классификации обычным `RidgeClassifier`, перебирая разные векторайзеры и токенайзеры. Наиболее интересно, рассматривать confusion matrix, как мы делали на практике. Поэтому используйте функцию `examine`. Добавляйте в начало ячейки макрос `%%time`, чтобы видеть как долго работает обучение, трансформации и токенизации.\n",
    "\n",
    "В практике мы дополнительно выводили список наиболее важных слов по мнению классификатора при принятии решений. Вы тоже должны вывести список 10 самых важных слов для каждого класса для каждого эксперимента. Это даже важнее, чем просто процент ошибок, так как по этим словам ва можете сказать, есть ли у модели хоть какое-то понимание текстов или нет.\n",
    "\n",
    "### Задание:\n",
    "\n",
    "- перебор `simple_tokenize_text`, `stem_tokenize_text`, `lemm_tokenize_text`\n",
    "- перебор `CountVectorizer`, `TfidfVectorizer`\n",
    "- используем `RidgeClassifier`\n",
    "- разбиваем выборку на тестовую и тренировочную в соотношении 60/40 с random_state=42 (train_test_split)\n",
    "- рисуем confusion_matrix\n",
    "- выводим топ-10 наиболее важных слов по категориям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Возможные проблемы\n",
    "\n",
    "Обучите векторайзер функцией `fit` на всем датасете `X`. Внимание, если у вас выполняется функция `fit` на всём датасете медленее, чем 30 секунд на `mobile i5 2.5GHz, 16GB Ram, SSD`, то вы написали неоптимально функцию токенизации.\n",
    "\n",
    "Если всё совсем плохо, например лемматизация очень медленно выполняется, то можете еще немного уменьшить датасет. Например, поставить `N=5000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите размер полученного словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покаижет наиболее важные слова для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_influential_words(clf, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Эксперименты с word2vec\n",
    "\n",
    "На практике мы рассмотрели продвинутую технологию анализа текстов: word2vec и doc2vec. Дополнительно про них вы можете посмотреть на видео: \n",
    "\n",
    "- [Мастер класс](https://www.youtube.com/watch?v=oBb9aFmp0Hs) по Gensim и [исходники](https://github.com/RaRe-Technologies/movie-plots-by-genre)\n",
    "- [Лекция](https://www.youtube.com/watch?v=hiDBnEyoZS4) по векторным представления слов в документах.\n",
    "\n",
    "### Задание\n",
    "\n",
    "- скачать предобученный word2vec [Google News pre-trained word2vec (1.5 GB)](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)\n",
    "- установить gensim\n",
    "- получите векторное представление текстов, применив токенайзер с лемматизацией. В качестве вектора текста будем использовать средний вектор всех слов текста. Это было описано в практике.\n",
    "- обучите RidgeClassifier\n",
    "- постройте confusion matrix\n",
    "- выведите наиболее близкие в векторном смысле слова к среднему вектору темы (это тоже было)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Doc2Vec\n",
    "\n",
    "Теперь обучаем doc2vec модель. Посмотрите сами сколько нужно выбрать итераций, чтобы получилось более менее приемлемое качество. В этот раз у нас данных достаточно много, но мы ограничены производительностю компьютера. \n",
    "\n",
    "Основная цель этого экспериента - получить сравнение близости тем(тэгов doc2vec) друг относительно друга, как мы делали с жанрами фильмов. Мы надеемся что комментарии про отличное вино сильно не похожи на комментарии про отвратительное. А может нет, doc2vec его знает. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# на подумать\n",
    "\n",
    "Если наша цель построить действительно мощный предсказатель. То нам стоит учитывать числа в текстах. Обычно их просто заменяют на символы `#`, но в данном случае они имеют значение. Возможно изх стоит рассматривать как отдельные числовые признаки. Вполне возможно в тексте будет много имен собственных, их тоже можно обрабатывать отдельно.\n",
    "\n",
    "Если вам будет интересна тема обработки текстов, то на kaggle есть соревнования: \n",
    "\n",
    "- https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "- https://www.kaggle.com/c/quora-question-pairs\n",
    "- https://www.kaggle.com/c/spooky-author-identification/\n",
    "\n",
    "Дополнительно можете почитать про [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
